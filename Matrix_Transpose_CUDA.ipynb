{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1saFFT7HTWJK",
        "outputId": "da4defed-b842-453d-ce88-0617f38bdb21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Matrix:\n",
            "     1     2     3     4\n",
            "     5     6     7     8\n",
            "     9    10    11    12\n",
            "    13    14    15    16\n",
            "\n",
            "Transposed Matrix (CPU):\n",
            "     1     5     9    13\n",
            "     2     6    10    14\n",
            "     3     7    11    15\n",
            "     4     8    12    16\n",
            "\n",
            "Transposed Matrix (GPU):\n",
            "     1     5     9    13\n",
            "     2     6    10    14\n",
            "     3     7    11    15\n",
            "     4     8    12    16\n",
            "\n",
            "CPU Time: 0 seconds\n",
            "GPU Time: 0.000162 seconds\n"
          ]
        }
      ],
      "source": [
        "# Write CUDA C++ code as a string\n",
        "cuda_code = r'''\n",
        "#include <iostream>\n",
        "#include <iomanip>\n",
        "#include <cuda_runtime.h>\n",
        "#include <sys/time.h>\n",
        "#define N 4  // Matrix size N x N\n",
        "\n",
        "__global__ void transposeGPU(float* input, float* output, int width) {\n",
        "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    if (x < width && y < width) {\n",
        "        int idx_in = y * width + x;\n",
        "        int idx_out = x * width + y;\n",
        "        output[idx_out] = input[idx_in];\n",
        "    }\n",
        "}\n",
        "\n",
        "void transposeCPU(float* input, float* output, int width) {\n",
        "    for (int i = 0; i < width; ++i)\n",
        "        for (int j = 0; j < width; ++j)\n",
        "            output[j * width + i] = input[i * width + j];\n",
        "}\n",
        "\n",
        "double getTimeDiff(timeval start, timeval end) {\n",
        "    return (double)(end.tv_sec - start.tv_sec) + (double)(end.tv_usec - start.tv_usec) / 1e6;\n",
        "}\n",
        "\n",
        "void printMatrix(const char* title, float* matrix, int width) {\n",
        "    std::cout << title << std::endl;\n",
        "    for (int i = 0; i < width * width; ++i) {\n",
        "        std::cout << std::setw(6) << matrix[i];\n",
        "        if ((i + 1) % width == 0) std::cout << std::endl;\n",
        "    }\n",
        "    std::cout << std::endl;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    float hostInput[N*N], hostOutputCPU[N*N], hostOutputGPU[N*N];\n",
        "    float *devInput, *devOutput;\n",
        "    timeval start, end;\n",
        "\n",
        "    // Initialize input matrix\n",
        "    for (int i = 0; i < N*N; ++i)\n",
        "        hostInput[i] = static_cast<float>(i + 1);\n",
        "\n",
        "    // CPU transpose\n",
        "    gettimeofday(&start, nullptr);\n",
        "    transposeCPU(hostInput, hostOutputCPU, N);\n",
        "    gettimeofday(&end, nullptr);\n",
        "    double cpuTime = getTimeDiff(start, end);\n",
        "\n",
        "    // Allocate memory for device\n",
        "    cudaMalloc((void**)&devInput, N*N*sizeof(float));\n",
        "    cudaMalloc((void**)&devOutput, N*N*sizeof(float));\n",
        "    cudaMemcpy(devInput, hostInput, N*N*sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Kernel configuration\n",
        "    dim3 threadsPerBlock(2, 2);  // 2x2 block size\n",
        "    dim3 numBlocks((N + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
        "                   (N + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
        "\n",
        "    // GPU transpose\n",
        "    gettimeofday(&start, nullptr);\n",
        "    transposeGPU<<<numBlocks, threadsPerBlock>>>(devInput, devOutput, N);\n",
        "    cudaDeviceSynchronize();  // Wait for GPU to finish\n",
        "    gettimeofday(&end, nullptr);\n",
        "    double gpuTime = getTimeDiff(start, end);\n",
        "\n",
        "    // Check for CUDA errors\n",
        "    cudaError_t err = cudaGetLastError();\n",
        "    if (err != cudaSuccess) {\n",
        "        std::cerr << \"CUDA error: \" << cudaGetErrorString(err) << std::endl;\n",
        "        return -1;\n",
        "    }\n",
        "\n",
        "    // Copy result back to host\n",
        "    cudaMemcpy(hostOutputGPU, devOutput, N*N*sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Display matrices\n",
        "    printMatrix(\"Original Matrix:\", hostInput, N);\n",
        "    printMatrix(\"Transposed Matrix (CPU):\", hostOutputCPU, N);\n",
        "    printMatrix(\"Transposed Matrix (GPU):\", hostOutputGPU, N);\n",
        "\n",
        "    std::cout << \"CPU Time: \" << cpuTime << \" seconds\" << std::endl;\n",
        "    std::cout << \"GPU Time: \" << gpuTime << \" seconds\" << std::endl;\n",
        "\n",
        "    // Free memory\n",
        "    cudaFree(devInput);\n",
        "    cudaFree(devOutput);\n",
        "    return 0;\n",
        "}\n",
        "'''\n",
        "\n",
        "# Save the CUDA code to a file\n",
        "with open(\"/content/matrix_transpose.cu\", \"w\") as f:\n",
        "    f.write(cuda_code)\n",
        "\n",
        "# Compile the CUDA code using nvcc\n",
        "!nvcc -arch=sm_75 -std=c++11 /content/matrix_transpose.cu -o /content/matrix_transpose\n",
        "\n",
        "# Run the compiled CUDA code\n",
        "!./matrix_transpose"
      ]
    }
  ]
}